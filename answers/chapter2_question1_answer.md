# Lexical Analysis Concepts

This document explains key terms related to lexical analysis: Lexical Analysis itself, Tokens, and Input Buffering.

---

## a) Lexical Analysis

**Definition:**
Lexical Analysis, also known as scanning or tokenization, is the **first phase of a compiler**. Its primary role is to read the stream of characters making up the source program and group them into meaningful sequences called **lexemes**. For each lexeme identified, the lexical analyzer generates a **token** and passes it to the next phase of the compiler, the syntax analyzer (parser).

In essence, the lexical analyzer acts as an interface between the raw source code and the parser, simplifying the parser's task by providing a structured sequence of tokens instead of a raw character stream.

**Functions of Lexical Analysis:**

1.  **Reading Input Characters:** It reads the source program character by character from left to right.
2.  **Identifying Lexemes:** It groups characters into lexemes, which are instances of tokens (e.g., `count`, `100`, `+`, `while`).
3.  **Generating Tokens:** For each lexeme, it produces a token representing its category (e.g., `IDENTIFIER`, `INTEGER_CONSTANT`, `ADD_OPERATOR`, `KEYWORD_WHILE`). A token typically consists of a token name (or code) and an optional attribute value (e.g., a pointer to the symbol table entry for an identifier, or the numeric value for a constant).
4.  **Stripping Out Whitespace and Comments:** It usually removes comments and whitespace (spaces, tabs, newlines) as they are generally not significant for the subsequent compilation phases, though they might be relevant for some preprocessors or pretty-printers.
5.  **Correlating Error Messages:** It can help correlate error messages generated by later phases with the source program (e.g., by keeping track of line numbers).
6.  **Handling Lexical Errors:** It detects and reports lexical errors, such as illegal characters or malformed tokens.

**Example:**
Consider the source code statement: `result = initial + rate * 10;`

The lexical analyzer would process this as follows:

1.  Reads `r`, `e`, `s`, `u`, `l`, `t`. Groups them into the lexeme `result`. Generates a token like `<IDENTIFIER, pointer to 'result' in symbol table>`.
2.  Reads space. Ignores it.
3.  Reads `=`. Groups it into the lexeme `=`. Generates a token like `<ASSIGN_OPERATOR>`.
4.  Reads space. Ignores it.
5.  Reads `i`, `n`, `i`, `t`, `i`, `a`, `l`. Groups them into the lexeme `initial`. Generates a token like `<IDENTIFIER, pointer to 'initial' in symbol table>`.
6.  Reads space. Ignores it.
7.  Reads `+`. Groups it into the lexeme `+`. Generates a token like `<ADD_OPERATOR>`.
8.  Reads space. Ignores it.
9.  Reads `r`, `a`, `t`, `e`. Groups them into the lexeme `rate`. Generates a token like `<IDENTIFIER, pointer to 'rate' in symbol table>`.
10. Reads space. Ignores it.
11. Reads `*`. Groups it into the lexeme `*`. Generates a token like `<MULTIPLY_OPERATOR>`.
12. Reads space. Ignores it.
13. Reads `1`, `0`. Groups them into the lexeme `10`. Generates a token like `<INTEGER_CONSTANT, 10>`.
14. Reads `;`. Groups it into the lexeme `;`. Generates a token like `<SEMICOLON>`.

---

## b) Tokens

**Definition:**
A **token** is a pair consisting of a *token name* and an optional *attribute value*. The token name is an abstract symbol representing a category of input strings (lexemes). For example, `IDENTIFIER`, `KEYWORD`, `INTEGER_CONSTANT`, `OPERATOR`, `PUNCTUATOR` are token names.

The attribute value, if present, provides additional information about the specific lexeme that was matched for that token. This attribute value is typically a pointer to the symbol table entry for an identifier, the numeric value of a constant, or the literal string for a string constant.

Tokens are the basic building blocks that the parser uses to construct the syntactic structure of the program.

**Types of Tokens:**
Programming languages typically have several categories of tokens:

1.  **Keywords:** Reserved words in the language that have special meanings (e.g., `if`, `else`, `while`, `int`, `return`).
    *   *Example Lexeme:* `while`
    *   *Example Token:* `<KEYWORD_WHILE>` (attribute value might be implicit or not needed if there's a unique token name for each keyword).
2.  **Identifiers:** Names used to identify variables, functions, classes, etc. (e.g., `count`, `totalAmount`, `myFunction`).
    *   *Example Lexeme:* `count`
    *   *Example Token:* `<IDENTIFIER, pointer to symbol table entry for 'count'>`.
3.  **Constants (Literals):** Fixed values, such as numbers, characters, or strings (e.g., `123`, `3.14`, `'a'`, `"hello"`).
    *   *Example Lexeme:* `123`
    *   *Example Token:* `<INTEGER_CONSTANT, 123>`.
    *   *Example Lexeme:* `"hello"`
    *   *Example Token:* `<STRING_LITERAL, pointer to stored string "hello">`.
4.  **Operators:** Symbols that perform operations (e.g., `+`, `-`, `*`, `/`, `=`, `==`, `&&`).
    *   *Example Lexeme:* `+`
    *   *Example Token:* `<ADD_OPERATOR>`.
    *   *Example Lexeme:* `==`
    *   *Example Token:* `<EQUALITY_OPERATOR>`.
5.  **Punctuators (Separators):** Symbols used to structure the program (e.g., `(`, `)`, `{`, `}`, `;`, `,`).
    *   *Example Lexeme:* `;`
    *   *Example Token:* `<SEMICOLON>`.

**Lexemes vs. Tokens:**
*   A **lexeme** is the actual sequence of characters in the source code that matches the pattern for a token.
*   A **token** is a classification of that lexeme.

For example:
*   Lexemes: `var1`, `counter`, `x`
*   Token (for all above): `IDENTIFIER` (with different attribute values for each)

*   Lexemes: `10`, `2500`, `0`
*   Token (for all above): `INTEGER_CONSTANT` (with different attribute values)

---

## c) Input Buffering

**Definition:**
Input buffering is a technique used by lexical analyzers to efficiently read and process the source program characters. Since reading one character at a time from the input source (e.g., a file on disk) can be slow due to the overhead of system calls, lexical analyzers often read a large block of characters into a buffer in memory at once.

The lexical analyzer then scans these characters from the buffer. This significantly reduces the number of I/O operations and speeds up the lexical analysis process.

**Purpose and Benefits:**

1.  **Efficiency:** Reduces the overhead associated with frequent system calls for reading single characters.
2.  **Lookahead:** Facilitates looking ahead at upcoming characters. Many languages require the lexer to look beyond the current character to correctly identify a lexeme. For example, to distinguish between `<` (less than), `<=` (less than or equal to), and `<<` (left shift), the lexer needs to see one or two characters beyond the `<`.

**Common Input Buffering Schemes:**

1.  **Single Buffer:** A block of N characters is read into a single buffer. Two pointers, `lexemeBegin` and `forward`, are typically used. `lexemeBegin` points to the start of the current lexeme being identified, and `forward` scans ahead to find the end of the lexeme. When `forward` reaches the end of the buffer, the entire buffer needs to be reloaded, which can be problematic if a lexeme spans across the buffer boundary.

2.  **Two-Buffer Scheme (Sentinels):** This is a more common and robust approach. Two buffers of equal size (say, N characters each) are used. 
    *   Characters are read into one buffer at a time. When the `forward` pointer reaches the end of one buffer, the other buffer is filled with the next block of characters from the input.
    *   A special character called a **sentinel** (which is not part of the source language alphabet, often `EOF` marker) is placed at the end of each buffer. 
    *   When the `forward` pointer encounters a sentinel, it knows it has reached the end of that buffer. 
        *   If it's the sentinel of the first buffer, it reloads the first buffer and moves `forward` to the beginning of the second buffer (which was already filled or gets filled now).
        *   If it's the sentinel of the second buffer, it reloads the second buffer and moves `forward` to the beginning of the first buffer.
        *   If it encounters an actual `EOF` from the input file while trying to fill a buffer, and then encounters the sentinel corresponding to that `EOF`, it signals the end of the input.

    **Advantages of the Two-Buffer Scheme with Sentinels:**
    *   **Handles Spanning Lexemes:** Lexemes can span across buffer boundaries without special case handling, as the lookahead mechanism continues smoothly into the next buffer.
    *   **Efficient Lookahead:** The check for the end of a buffer (by testing for the sentinel) can be combined with the character processing loop, often requiring only one check per character advanced by `forward`.

**Example of Two-Buffer Scheme:**
Imagine two buffers, B1 and B2, each of size 4096 characters.

```
Buffer B1: [ c h a r   v a ... r _ 1 EOF_sentinel ]
                  ^lexemeBegin  ^
                                forward

Buffer B2: [ =   'x' ; ...           EOF_sentinel ]
```

*   Initially, B1 is filled. `lexemeBegin` and `forward` start at the beginning of B1.
*   `forward` moves, identifying lexemes. `lexemeBegin` is updated to the start of the new lexeme.
*   If `forward` reaches `EOF_sentinel` in B1, the system fills B2 (if not already filled or if it was the previously used buffer). `forward` then continues into B2.
*   The process alternates between buffers.

Input buffering is a crucial low-level optimization that makes lexical analysis practical for large source files. 